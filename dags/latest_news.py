from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import urllib.request
import urllib.parse
import json
import requests
import re
import html 
import FinanceDataReader as fdr

# ë„¤ì´ë²„ API ì„¤ì •
CLIENT_ID = "azhP2a68ejoD_N1Bwp55"
CLIENT_SECRET = "I9LYuloz92"

# Django API ì£¼ì†Œ
DJANGO_API_URL = "http://django:8000/api/latest-news/"

def clean_html(text):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', text)
    
    return html.unescape(cleantext)

def analyze_sentiment_basic(text):
    """ê°„ë‹¨ ê°ì„± ë¶„ì„"""
    positive_keywords = ['ê¸‰ë“±', 'ê°•ì„¸', 'ìƒìŠ¹', 'í˜¸ì¬', 'ëŒ€ë°•', 'ì„±ì¥', 'ìµœê³ ', 'ìˆ˜ì£¼', 'í‘ì', 'ëŒíŒŒ', 'ê¸°ëŒ€']
    negative_keywords = ['ê¸‰ë½', 'ì•½ì„¸', 'í•˜ë½', 'ì•…ì¬', 'ì ì', 'ìš°ë ¤', 'ë°”ë‹¥', 'ì†ì‹¤', 'ë‘”í™”', 'ìœ„ê¸°', 'ë¶ˆì•ˆ']
    text = text.replace(" ", "")
    if any(keyword in text for keyword in positive_keywords): return 'positive'
    elif any(keyword in text for keyword in negative_keywords): return 'negative'
    else: return 'neutral'

def extract_source_from_url(url):
    """URLì—ì„œ ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ì¶”ì¶œ"""
    try:
        parsed = urllib.parse.urlparse(url)
        return parsed.netloc.replace('www.', '')
    except:
        return "Internet News"

# ğŸ‘‡ [ì¶”ê°€] ë‰´ìŠ¤ í˜ì´ì§€ì— ì§ì ‘ ì ‘ì†í•´ì„œ og:image (ëŒ€í‘œ ì´ë¯¸ì§€) ì¶”ì¶œ
def extract_og_image(url):
    try:
        # 1ì´ˆ ì•ˆì— ì‘ë‹µ ì—†ìœ¼ë©´ í¬ê¸° (ì†ë„ ì €í•˜ ë°©ì§€)
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=1.5)
        
        if response.status_code == 200:
            html = response.text
            # <meta property="og:image" content="..."> íŒ¨í„´ ì°¾ê¸°
            match = re.search(r'<meta\s+property=["\']og:image["\']\s+content=["\'](.*?)["\']', html, re.IGNORECASE)
            if match:
                return match.group(1) # ì´ë¯¸ì§€ URL ë°˜í™˜
    except Exception:
        pass # ì´ë¯¸ì§€ ì—†ê±°ë‚˜ ì ‘ì† ì‹¤íŒ¨í•˜ë©´ ì¿¨í•˜ê²Œ íŒ¨ìŠ¤
    return None

def get_request_url(url):
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", CLIENT_ID)
    req.add_header("X-Naver-Client-Secret", CLIENT_SECRET)
    try:
        response = urllib.request.urlopen(req)
        if response.getcode() == 200:
            return response.read().decode('utf-8')
    except Exception as e:
        print(f"[ERROR] API Request Failed: {e}")
        return None

def get_naver_search(keyword, start, display):
    base = "https://openapi.naver.com/v1/search/news.json"
    params = f"?query={urllib.parse.quote(keyword)}&start={start}&display={display}&sort=date"
    url = base + params
    result = get_request_url(url)
    return json.loads(result) if result else None

def get_kospi_stock_names(limit=None):
    print(">>> KOSPI ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë”©...")
    krx_stocks = fdr.StockListing('KRX')              
    kospi_stocks = krx_stocks[krx_stocks['Market'] == 'KOSPI']  
    names = kospi_stocks['Name'].dropna().tolist()
    if limit: names = names[:limit]
    print(f">>> ì´ {len(names)}ê°œì˜ ì¢…ëª© í‚¤ì›Œë“œ ì‚¬ìš©")
    return names

def crawl_and_send_to_django(**context):
    params = context.get("params", {})
    keywords = params.get("keywords")

    if isinstance(keywords, str):
        keywords = [k.strip() for k in keywords.split(",") if k.strip()]

    if not keywords:
        limit = params.get("limit")  
        keywords = get_kospi_stock_names(limit=limit)

    now_kst = datetime.utcnow() + timedelta(hours=9)
    target_date = now_kst.strftime("%Y-%m-%d")
    print(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {target_date}")
    
    display = 3
    start = 1
    total_success = 0
    total_fail = 0

    for keyword in keywords:
        print(f"\n====== ğŸ” í‚¤ì›Œë“œ: {keyword} ======")
        json_data = get_naver_search(keyword, start, display)

        success_count = 0
        fail_count = 0

        if json_data and "items" in json_data:
            for item in json_data["items"]:
                # 1. ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
                try:
                    raw_date = item["pubDate"]
                    dt_obj = datetime.strptime(raw_date, "%a, %d %b %Y %H:%M:%S +0900")
                    article_date = dt_obj.strftime("%Y-%m-%d")
                    
                    # â­ [ìˆ˜ì •] ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜ (YYYY-MM-DDTHH:MM:SS)
                    full_date_time = dt_obj.isoformat()
                except:
                    continue

                if article_date != target_date:
                    continue

                # 2. ë°ì´í„° ì •ì œ
                title_clean = clean_html(item["title"])
                description_clean = clean_html(item["description"])
                news_link = item.get("originallink") or item.get("link")
                
                # â­ [ì•ˆì „ì¥ì¹˜] ë¹ˆ ë¬¸ìì—´ì´ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì²˜ë¦¬
                if not title_clean: title_clean = "ì œëª© ì—†ìŒ"
                if not description_clean: description_clean = "ë‚´ìš© ì—†ìŒ"

                # 3. ë°ì´í„° ì „ì†¡ ì¤€ë¹„
                image_url = extract_og_image(news_link)

                payload = {
                    "title": title_clean[:255], # ê¸¸ì´ ì œí•œ
                    "body": description_clean,
                    "news_collection_date": full_date_time,
                    "url": news_link,
                    "views": 0,
                    "company_name": keyword,
                    "source": extract_source_from_url(news_link)[:50],
                    "sentiment": analyze_sentiment_basic(title_clean),
                    "image_url": image_url
                }
                
                # â­ [í•µì‹¬] JSONìœ¼ë¡œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°›ê¸° ìœ„í•œ í—¤ë”
                headers = {
                    "Content-Type": "application/json",
                    "Accept": "application/json"
                }

                try:
                    response = requests.post(DJANGO_API_URL, json=payload, headers=headers)
                    if response.status_code == 201:
                        success_count += 1
                    else:
                        # ì´ì œ ë¡œê·¸ì— HTMLì´ ì•„ë‹ˆë¼ {"title": ["This field is required"]} ì²˜ëŸ¼ ë‚˜ì˜µë‹ˆë‹¤!
                        print(f"âŒ ì‹¤íŒ¨ ({response.status_code}): {response.text}")
                        fail_count += 1
                except Exception as e:
                    print(f"ğŸ’¥ ì „ì†¡ ì—ëŸ¬: {e}")
                    fail_count += 1

        print(f"â¡ '{keyword}' ê²°ê³¼: ì„±ê³µ {success_count} / ì‹¤íŒ¨ {fail_count}")
        total_success += success_count
        total_fail += fail_count

    print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼: ì„±ê³µ {total_success} / ì‹¤íŒ¨ {total_fail}")

default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

with DAG(
    dag_id="naver_news_to_postgres",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@daily",
    catchup=False,
    default_args=default_args,
    params={"limit": 100}
) as dag:
    task = PythonOperator(
        task_id="crawl_and_send_news",
        python_callable=crawl_and_send_to_django,
        provide_context=True,
    )